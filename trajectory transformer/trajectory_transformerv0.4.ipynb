{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "72e7a37a",
   "metadata": {
    "id": "72e7a37a"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import bisect\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 \n",
    "block_size = 32 \n",
    "max_iters = 5000\n",
    "epochs = 5\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "st_dims = 4\n",
    "ac_dims = 1\n",
    "rw_dims = 1\n",
    "rtg_dims= 1\n",
    "traj_len = st_dims + ac_dims + rw_dims + rtg_dims\n",
    "pattern = [100, 100, 100, 100, 2, 1, 100]\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "out_vocab = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "408a7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cartpole_play.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "ed583b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>obs1</th>\n",
       "      <th>obs2</th>\n",
       "      <th>obs3</th>\n",
       "      <th>obs4</th>\n",
       "      <th>actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewardstg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048681</td>\n",
       "      <td>0.233331</td>\n",
       "      <td>-0.044224</td>\n",
       "      <td>-0.267353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053347</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>-0.049571</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054124</td>\n",
       "      <td>-0.155511</td>\n",
       "      <td>-0.049350</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051014</td>\n",
       "      <td>-0.349895</td>\n",
       "      <td>-0.043596</td>\n",
       "      <td>0.564419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044016</td>\n",
       "      <td>-0.154190</td>\n",
       "      <td>-0.032307</td>\n",
       "      <td>0.258327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36762</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36764</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36765</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36766</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36767 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode      obs1      obs2      obs3      obs4  actions  rewards  \\\n",
       "0          0.0  0.048681  0.233331 -0.044224 -0.267353      1.0      1.0   \n",
       "1          0.0  0.053347  0.038867 -0.049571  0.011060      0.0      1.0   \n",
       "2          0.0  0.054124 -0.155511 -0.049350  0.287700      0.0      1.0   \n",
       "3          0.0  0.051014 -0.349895 -0.043596  0.564419      0.0      1.0   \n",
       "4          0.0  0.044016 -0.154190 -0.032307  0.258327      1.0      1.0   \n",
       "...        ...       ...       ...       ...       ...      ...      ...   \n",
       "36762   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36763   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36764   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36765   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36766   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "\n",
       "       rewardstg  \n",
       "0           66.0  \n",
       "1           65.0  \n",
       "2           64.0  \n",
       "3           63.0  \n",
       "4           62.0  \n",
       "...          ...  \n",
       "36762        4.0  \n",
       "36763        3.0  \n",
       "36764        2.0  \n",
       "36765        1.0  \n",
       "36766       -0.0  \n",
       "\n",
       "[36767 rows x 8 columns]"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rename = {'0':'episode', '1':'obs1', '2':'obs2', '3':'obs3', '4':'obs4', '5':'actions', '6':'rewards', '7':'rewardstg'}\n",
    "data = data.rename(rename, axis=1)\n",
    "data.drop(columns='Unnamed: 0', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "a0545c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = data[['obs1', 'obs2', 'obs3', 'obs4']]\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']-1\n",
    "rewardstg = data['rewardstg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ff800",
   "metadata": {},
   "source": [
    "# Discretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be119f5d",
   "metadata": {},
   "source": [
    "Here's the deal: we need to discretize continuous actions to a vocabulary size ```vocab = 100```. Moreover, rewards to go is up to 150, so we also need to shrink that to a vocabulary to a size ```100```. For that, we use sklearns KBinsDiscretizer, which discretizes a input into bins. However, once we discretize, we will have bins identified from 0 to 99. If we want to use these as tokens, we will run into the problem that the token 1 for state and the token 1 for action are treated the same way by the input layer. We do not want that. We need to differentiate those tokens.  The following class allows to discretize and differentiate tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "d6ee3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class will work with numpy arrays as inputs, except fit_transform\n",
    "class DataDiscretizer:\n",
    "    def __init__(self, vocab, traj_len, encode='ordinal', strategy='uniform'):\n",
    "        self.vocab = vocab\n",
    "        self.traj_len = traj_len\n",
    "        self.ob_enc = KBinsDiscretizer(vocab, encode=\"ordinal\", strategy='uniform')\n",
    "        self.rtg_enc = KBinsDiscretizer(vocab, encode=\"ordinal\", strategy=\"uniform\")\n",
    "        \n",
    "    def fit_transform(self, observations, actions, rewards, rewardstg):\n",
    "        self.dif = np.array([i*out_vocab for i in range(traj_len)])\n",
    "        sim_ob_tok = pd.DataFrame(self.ob_enc.fit_transform(observations.values))\n",
    "        sim_rtg_tok = pd.DataFrame(self.rtg_enc.fit_transform(rewardstg.values.reshape(-1, 1)))\n",
    "        sim_tok = pd.concat([sim_ob_tok, actions, rewards, sim_rtg_tok], axis=1)\n",
    "        dif_tok = sim_tok.copy() + self.dif\n",
    "        self.total_vocab = dif_tok.max().max()\n",
    "        return dif_tok.values.reshape(-1).astype(int), sim_tok.values.reshape(-1).astype(int)\n",
    "        \n",
    "    def discretize_observation(self, obs):\n",
    "        return self.ob_enc.transform(obs.reshape(1, -1)).reshape(-1)\n",
    "    \n",
    "    def discretize_rtg(self, rtg):\n",
    "        return self.rtg_enc.transform(rtg.reshape(1, -1)).reshape(-1)\n",
    "        \n",
    "    def discretize_trajectory(self, traj):\n",
    "        obs = traj[:st_dims]\n",
    "        anr = traj[st_dims:-1]\n",
    "        rtg = traj[-1:]\n",
    "        sim_obs_tok = self.discretize_observation(obs)\n",
    "        sim_rtg_tok = self.discretize_rtg(rtg)\n",
    "        sim_tok = np.concatenate([sim_obs_tok, anr, sim_rtg_tok])\n",
    "        dif_tok = sim_tok.copy() + self.dif\n",
    "        return sim_tok.astype(int), dif_tok.astype(int)\n",
    "    \n",
    "    def similar_to_different(self, tok, mod):\n",
    "        mod = mod%self.traj_len\n",
    "        return tok+self.dif[mod]\n",
    "    \n",
    "    def different_to_similar(self, tok, mod):\n",
    "        mod = mod%self.traj_len\n",
    "        return tok-self.dif[mod]\n",
    "    \n",
    "    def get_raw(self, tok, mod):\n",
    "        tok -= self.dif[mod] \n",
    "        if mod < st_dims:\n",
    "            assert NotImplemented\n",
    "        elif mod < st_dims + ac_dims + rw_dims:\n",
    "            return tok\n",
    "        else:\n",
    "            return self.rtg_enc.inverse_transform(tok.reshape(1, -1)).reshape(-1)[0]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.total_vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "9432bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DataDiscretizer(out_vocab, traj_len)\n",
    "dif, sim = dd.fit_transform(observations, actions, rewards, rewardstg)\n",
    "inp_vocab = int(dd.get_vocab_size())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "5ca92e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>obs1</th>\n",
       "      <th>obs2</th>\n",
       "      <th>obs3</th>\n",
       "      <th>obs4</th>\n",
       "      <th>actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewardstg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048681</td>\n",
       "      <td>0.233331</td>\n",
       "      <td>-0.044224</td>\n",
       "      <td>-0.267353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053347</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>-0.049571</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054124</td>\n",
       "      <td>-0.155511</td>\n",
       "      <td>-0.049350</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051014</td>\n",
       "      <td>-0.349895</td>\n",
       "      <td>-0.043596</td>\n",
       "      <td>0.564419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044016</td>\n",
       "      <td>-0.154190</td>\n",
       "      <td>-0.032307</td>\n",
       "      <td>0.258327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36762</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36764</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36765</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36766</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36767 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode      obs1      obs2      obs3      obs4  actions  rewards  \\\n",
       "0          0.0  0.048681  0.233331 -0.044224 -0.267353      1.0      1.0   \n",
       "1          0.0  0.053347  0.038867 -0.049571  0.011060      0.0      1.0   \n",
       "2          0.0  0.054124 -0.155511 -0.049350  0.287700      0.0      1.0   \n",
       "3          0.0  0.051014 -0.349895 -0.043596  0.564419      0.0      1.0   \n",
       "4          0.0  0.044016 -0.154190 -0.032307  0.258327      1.0      1.0   \n",
       "...        ...       ...       ...       ...       ...      ...      ...   \n",
       "36762   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36763   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36764   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36765   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36766   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "\n",
       "       rewardstg  \n",
       "0           66.0  \n",
       "1           65.0  \n",
       "2           64.0  \n",
       "3           63.0  \n",
       "4           62.0  \n",
       "...          ...  \n",
       "36762        4.0  \n",
       "36763        3.0  \n",
       "36764        2.0  \n",
       "36765        1.0  \n",
       "36766       -0.0  \n",
       "\n",
       "[36767 rows x 8 columns]"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "ddb628ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = torch.tensor(dif, dtype=torch.long)\n",
    "sim = torch.tensor(sim, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6680a5",
   "metadata": {},
   "source": [
    "# Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "f0e30a66",
   "metadata": {
    "id": "f0e30a66"
   },
   "outputs": [],
   "source": [
    "#data is heavily identified by its index\n",
    "#since it determines whther is a state (pos 0, 1, 2, 3)\n",
    "#and action (4), a reward (5), or rewrad to go(6)\n",
    "#so in the follwoing funciton we also output the position\n",
    "#of the first token.\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(dif) - block_size, (batch_size,))\n",
    "    x = torch.stack([dif[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([sim[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y, ix%traj_len\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    tt.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        x, y, mod = get_batch()\n",
    "        logits, loss = tt(x, mod, targets=y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    tt.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "890143c1",
   "metadata": {
    "id": "890143c1"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(3*block_size, 3*block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca92667",
   "metadata": {},
   "source": [
    "The only difference between the following transformer and the vanilla transformer is that we have a generate_mask function, thats masks out ilegal actions or rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "8208ce49",
   "metadata": {
    "id": "8208ce49"
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inp_embedding = nn.Embedding(inp_vocab, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, out_vocab)\n",
    "\n",
    "    def forward(self, sequence, mods, targets=None):\n",
    "        vocab_embedding = self.inp_embedding(sequence) #(B,T,C)\n",
    "        B, T, C = vocab_embedding.shape\n",
    "        pos_encoding = self.position_embedding_table(torch.arange(T, device=device)) #(T,C)\n",
    "        x = vocab_embedding + pos_encoding #(B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) \n",
    "\n",
    "        #MODIFICATON\n",
    "        preds = self.lm_head(x) # (B,T,vocab)\n",
    "        #make all the imporoper values ilegal\n",
    "        mask = self.generate_mask(mods, T)\n",
    "        preds = preds.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = preds.shape\n",
    "            preds = preds.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(preds, targets)\n",
    "\n",
    "        return preds, loss\n",
    "    \n",
    "    def generate_mask(self, mods, T):\n",
    "        \n",
    "        mask = torch.stack(\n",
    "        [torch.stack(\n",
    "            [(torch.arange(out_vocab)) >= pattern[(i+mod+1)%traj_len] #true if value should be masked\n",
    "                 for i in range(T)])  #we want to mask up to sequence length \n",
    "                     for mod in mods]) #we want to mask specific positions \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "57e159f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TrajectoryTransformer()\n",
    "tt.to(device)\n",
    "optimizer = torch.optim.AdamW(tt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "dcabb811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tt =TrajectoryTransformer()\n",
    "x, y, mod = get_batch()\n",
    "preds, _ = tt(x, mod, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "4634054e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4634054e",
    "outputId": "9e0c0449-8144-4b10-85b6-2b2347227496",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.5497\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    for iteration in range(1):\n",
    "      # every once in a while evaluate the loss on train and val sets\n",
    "        if iteration % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iteration}: train loss {losses:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        x, y, mods = get_batch()\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = tt(x, mods,targets=y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f36ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "7ff95441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62b474",
   "metadata": {},
   "source": [
    "Wont remove the porints cuz they are extremly useful for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "jRzYNg9m3cRk",
   "metadata": {
    "id": "jRzYNg9m3cRk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#We must assume that the context is a trajectory beginning with SAR\n",
    "def beam_search(model, context, st_width=3, ac_width=2):\n",
    "    window_by_index = [st_width] * st_dims + [ac_width] * ac_dims + \\\n",
    "                                        [1]*rw_dims + [1]*rtg_dims\n",
    "    top_context = np.array([context])\n",
    "    top_prob = np.array([0])\n",
    "\n",
    "    for i in range(traj_len):\n",
    "#         print(i, top_context.shape, top_prob.shape)\n",
    "        sqr_context = []\n",
    "        sqr_probs = []\n",
    "        k = window_by_index[i%traj_len]\n",
    "\n",
    "        for context, prob in zip(top_context, top_prob):\n",
    "            mod = 0 if len(context) < block_size else len(context)%traj_len\n",
    "            inp = torch.tensor(context[-block_size:], device=device).unsqueeze(0)\n",
    "            logits, _ = model(inp, mods=torch.tensor(mod).reshape(1, 1))\n",
    "#             print(logits.shape)\n",
    "            logits = logits[0, -1, :]\n",
    "            logits = torch.log_softmax(logits, dim=0).detach().cpu().numpy()\n",
    "#             print('--------------------------logits----------------------------------')\n",
    "#             print(logits)\n",
    "#             print('--------------------------logits----------------------------------')\n",
    "            \n",
    "            k_logits_idx = np.argpartition(logits, -k)[-k:]\n",
    "            sqr_probs.extend(logits[k_logits_idx] + prob)\n",
    "\n",
    "            k_tokens = [dd.similar_to_different(tk, i) for tk in k_logits_idx]\n",
    "            \n",
    "            sqr_context.extend(np.concatenate([context, np.array(tok).reshape(1,)]) for tok in k_tokens)\n",
    "\n",
    "#         print('--------------------------sqr----------------------------------')\n",
    "#         print('sqr', sqr_context, sqr_probs)\n",
    "#         print('-------------------------sqr----------------------------------')\n",
    "#         print(np.shape(sqr_context), np.shape(sqr_probs))\n",
    "\n",
    "        if i%traj_len < st_dims:\n",
    "            idxs = np.argsort(sqr_probs)[-k:]\n",
    "        else:\n",
    "            idxs = np.argsort(sqr_probs)\n",
    "\n",
    "        top_context = np.array(sqr_context)[idxs]\n",
    "        top_prob = np.array(sqr_probs)[idxs]\n",
    "#         print(top_context, top_prob)\n",
    "       \n",
    "    rw_idx, rewards = get_reward_idxs(top_context)\n",
    "    \n",
    "    \n",
    "    return top_context[rw_idx], top_prob[rw_idx], rewards\n",
    "\n",
    "def extract_action(trajectory):\n",
    "    return trajectory[st_dims::trajectory_length]\n",
    "\n",
    "def get_reward_idxs(trajectories):\n",
    "    rewards = [dd.get_raw(traj[-2], 5) + dd.get_raw(traj[-1], 6) for traj in trajectories]\n",
    "    return np.argsort(rewards), np.sort(rewards)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "c091af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_beam_search():\n",
    "    top_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "-fKMpJSysfAQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "-fKMpJSysfAQ",
    "outputId": "d5088ec9-f78f-4fa5-ab95-33fff13bbfd5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 7) (1,)\n",
      "1 (3, 8) (3,)\n",
      "2 (3, 9) (3,)\n",
      "3 (3, 10) (3,)\n",
      "4 (3, 11) (3,)\n",
      "5 (6, 12) (6,)\n",
      "6 (6, 13) (6,)\n"
     ]
    }
   ],
   "source": [
    "top_beams_context,  top_beams_prob, rewards= beam_search(tt, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e8ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9a511b3",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "fc26b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_up_step():\n",
    "    action = 0\n",
    "    state, reward, terminated, _, _ = env.step(action)\n",
    "    to_encode = concat_traj(state, action, reward, desired_reward)\n",
    "#     print(to_encode)\n",
    "    _, context = dd.discretize_trajectory(to_encode)\n",
    "\n",
    "    return context\n",
    "\n",
    "def concat_traj(state, action, reward, desired_reward):\n",
    "    state_ = state.reshape(-1)\n",
    "    action_ = np.array(action).reshape(-1)\n",
    "    reward_ = np.array(reward).reshape(-1)\n",
    "    desired_reward_ = np.array(desired_reward).reshape(-1)\n",
    "    return np.concatenate([state_, action_, reward_, desired_reward_])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "a778261b",
   "metadata": {
    "id": "a778261b"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 150 \n",
    "\n",
    "def testing(model):\n",
    "    total_reward = 0.0\n",
    "    desired_reward = 120\n",
    "\n",
    "    _= env.reset(seed=None)\n",
    "    terminated = False\n",
    "    time = 0\n",
    "    context = warm_up_step()\n",
    " \n",
    "    while not terminated and time <= 500:\n",
    "        context_lar, probs, rews = beam_search(tt, context)\n",
    "        predicted_traj = context_lar[-1][-traj_len:]\n",
    "        \n",
    "        action_tok = predicted_traj[4]\n",
    "        action = dd.different_to_similar(action_tok, 4)\n",
    "        \n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        desired_reward -= reward\n",
    "        \n",
    "        to_encode = concat_traj(state, action, reward, desired_reward)\n",
    "        _, context = dd.discretize_trajectory(to_encode)\n",
    "        \n",
    "#         print('+++++', context_lar)\n",
    "#         print('-----', context, '\\n')\n",
    "        \n",
    "        time += 1\n",
    "\n",
    "    print(\"total_reward = {}\".format(total_reward))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "a085433e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward = 12.0\n",
      "total_reward = 26.0\n",
      "total_reward = 22.0\n",
      "total_reward = 62.0\n",
      "total_reward = 99.0\n",
      "total_reward = 24.0\n",
      "total_reward = 43.0\n",
      "total_reward = 117.0\n",
      "total_reward = 38.0\n",
      "total_reward = 24.0\n",
      "total_reward = 47.0\n",
      "total_reward = 30.0\n",
      "total_reward = 34.0\n",
      "total_reward = 45.0\n",
      "total_reward = 71.0\n",
      "total_reward = 49.0\n",
      "total_reward = 39.0\n",
      "total_reward = 24.0\n",
      "total_reward = 16.0\n",
      "total_reward = 43.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    testing(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "abd26ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = TrajectoryTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddde2c",
   "metadata": {
    "id": "b6ddde2c"
   },
   "source": [
    "# Experimenting maybe we need it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965775d1",
   "metadata": {
    "id": "965775d1"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(traj):\n",
    "    observations = kbd_observations.transform(np.array(traj[0:4]).reshape(1, -1)).reshape(-1)\n",
    "    action = np.array(traj[4]).reshape(-1)\n",
    "    reward = np.array(traj[5]).reshape(-1)\n",
    "    rewardtg = kbd_rewardstg.transform(np.array(traj[6]).reshape(1, -1)).reshape(-1)\n",
    "    tokenized = np.concatenate([observations, action, reward, rewardtg])\n",
    "    tokenized += prevs \n",
    "    return tokenized.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.array([0.001888, 0.033522, -0.041096, -0.040241, 1.0, 1.0, 21.0])\n",
    "to_tokens(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5146cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "de5146cf",
    "outputId": "5e12ab43-20c1-49c4-eba3-f274d74ae709",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing(tt, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c5cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f525a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38be22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "\n",
    "test[1:3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10555083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "\n",
    "result = input_array[np.arange(0, len(input_array), 7)].reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = 0\n",
    "st_mask = [(x+mod)%7 < 4 for x in range(11)]\n",
    "ac_mask = [(x+mod)%7 == 4 for x in range(33)]\n",
    "rw_mask = [(x+mod)%7 == 5 for x in range(33)]\n",
    "rtg_mask= [(x+mod)%7 == 6 for x in range(33)]\n",
    "\n",
    "test = np.array(range(11))\n",
    "print(test[st_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3085a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll = nn.Linear(5, 2)\n",
    "inp = torch.tensor(np.random.rand(11, 5), dtype=torch.float)\n",
    "out = ll(inp)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "out[st_mask] = torch.zeros(st_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a26ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373168c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mods = np.array([0, 1, 2])  # Replace with your desired tensor of mods\n",
    "\n",
    "length = 14  # Length of the masks\n",
    "\n",
    "st_mask = torch.stack([(torch.arange(length) + mod) % 7 < 4 for mod in mods])\n",
    "ac_mask = torch.stack([(torch.arange(length) + mod) % 7 == 4 for mod in mods])\n",
    "rw_mask = torch.stack([(torch.arange(length) + mod) % 7 == 5 for mod in mods])\n",
    "rtg_mask = torch.stack([(torch.arange(length) + mod) % 7 == 6 for mod in mods])\n",
    "\n",
    "test = torch.tensor(np.random.rand(3, 14, 5), dtype=torch.float)\n",
    "# print(test)\n",
    "print(test.shape, st_mask.shape)\n",
    "# print(test[rtg_mask].reshape(3, -1, 5))\n",
    "# print(test[rtg_mask].shape)\n",
    "\n",
    "test[rtg_mask] = torch.zeros((6, 5), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47272442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[st_mask] = torch.zeros(st_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4eff02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42089c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "msf23o-hrbzC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "msf23o-hrbzC",
    "outputId": "42bd7b40-debc-404a-889a-8a5cbda6eaec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(save_path)\n",
    "tt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "nVXFx1cDsGke",
   "metadata": {
    "id": "nVXFx1cDsGke"
   },
   "outputs": [],
   "source": [
    "context = dif.view(-1, traj_len)[100].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88ea9c0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[48, 34, 64, 66,  1,  0,  6]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context - dd.dif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dd215354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 100])\n"
     ]
    }
   ],
   "source": [
    "preds, _ = tt(context, [0])\n",
    "print(preds.shape)\n",
    "toks = torch.argmax(preds, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a5cfca1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41, 67, 68,  1,  0,  4, 47]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7982a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "b1db893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DataDiscretizer(100, 7)\n",
    "dif, sim = dd.fit_transform(observations, actions, rewards, rewardstg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "b9634dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.324999999999996\n"
     ]
    }
   ],
   "source": [
    "t = dd.get_raw(642, 6)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "fb47d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dif.reshape(-1,traj_len)[5].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "fba00265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 56, 149, 245, 350, 401, 500, 640])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "3564c29e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56 149 245 350 401 500 640  56 145 245 355 401 500 640]\n",
      " [ 56 149 245 350 401 500 640  56 145 245 354 401 500 640]\n",
      " [ 56 149 245 350 401 500 640  56 153 245 345 400 500 640]\n",
      " [ 56 149 245 350 401 500 640  56 145 245 354 400 500 640]\n",
      " [ 56 149 245 350 401 500 640  56 145 245 355 400 500 640]\n",
      " [ 56 149 245 350 401 500 640  56 153 245 345 401 500 640]] [-16.75159   -14.50677    -8.491723   -2.0663614  -2.05692    -1.7910172]\n"
     ]
    }
   ],
   "source": [
    "print(top_beams_context, top_beams_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "5f915d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60.345, 60.345, 60.345, 60.345, 60.345, 60.345]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22782aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "67112234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.06963874e-03 -2.14497119e-01  1.95431262e-02  2.52352864e-01\n",
      "  0.00000000e+00  1.00000000e+00  1.20000000e+02]\n",
      "[-0.00306964 -0.21449712  0.01954313  0.25235286] [0. 1.] [120.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 53, 144, 254, 354, 400, 501, 680])"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_reward = 120\n",
    "env.reset()\n",
    "warm_up_step()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trajectory",
   "language": "python",
   "name": "trajectory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

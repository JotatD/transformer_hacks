{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72e7a37a",
   "metadata": {
    "id": "72e7a37a"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import bisect\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 \n",
    "block_size = 20 \n",
    "max_iters = 5000\n",
    "epochs = 5\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "st_dims = 4\n",
    "ac_dims = 1\n",
    "rw_dims = 1\n",
    "rtg_dims= 1\n",
    "\n",
    "traj_len = 7\n",
    "\n",
    "rw_cho = 1+1\n",
    "ac_cho = 2\n",
    "rtg_cho = 150+1\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "vocab = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408a7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cartpole_play.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed583b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>obs1</th>\n",
       "      <th>obs2</th>\n",
       "      <th>obs3</th>\n",
       "      <th>obs4</th>\n",
       "      <th>actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewardstg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048681</td>\n",
       "      <td>0.233331</td>\n",
       "      <td>-0.044224</td>\n",
       "      <td>-0.267353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053347</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>-0.049571</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054124</td>\n",
       "      <td>-0.155511</td>\n",
       "      <td>-0.049350</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051014</td>\n",
       "      <td>-0.349895</td>\n",
       "      <td>-0.043596</td>\n",
       "      <td>0.564419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044016</td>\n",
       "      <td>-0.154190</td>\n",
       "      <td>-0.032307</td>\n",
       "      <td>0.258327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36762</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36764</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36765</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36766</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36767 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode      obs1      obs2      obs3      obs4  actions  rewards  \\\n",
       "0          0.0  0.048681  0.233331 -0.044224 -0.267353      1.0      1.0   \n",
       "1          0.0  0.053347  0.038867 -0.049571  0.011060      0.0      1.0   \n",
       "2          0.0  0.054124 -0.155511 -0.049350  0.287700      0.0      1.0   \n",
       "3          0.0  0.051014 -0.349895 -0.043596  0.564419      0.0      1.0   \n",
       "4          0.0  0.044016 -0.154190 -0.032307  0.258327      1.0      1.0   \n",
       "...        ...       ...       ...       ...       ...      ...      ...   \n",
       "36762   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36763   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36764   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36765   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36766   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "\n",
       "       rewardstg  \n",
       "0           66.0  \n",
       "1           65.0  \n",
       "2           64.0  \n",
       "3           63.0  \n",
       "4           62.0  \n",
       "...          ...  \n",
       "36762        4.0  \n",
       "36763        3.0  \n",
       "36764        2.0  \n",
       "36765        1.0  \n",
       "36766       -0.0  \n",
       "\n",
       "[36767 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rename = {'0':'episode', '1':'obs1', '2':'obs2', '3':'obs3', '4':'obs4', '5':'actions', '6':'rewards', '7':'rewardstg'}\n",
    "data = data.rename(rename, axis=1)\n",
    "data.drop(columns='Unnamed: 0', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0545c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = data[['obs1', 'obs2', 'obs3', 'obs4']]\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']\n",
    "rewardstg = data['rewardstg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6ee3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class will work with numpy arrays as inputs, except fit_transform\n",
    "class DataDiscretizer:\n",
    "    def __init__(self, vocab, traj_len, encode='ordinal', strategy='uniform'):\n",
    "        self.vocab = vocab\n",
    "        self.traj_len = traj_len\n",
    "        self.ob_enc = KBinsDiscretizer(vocab, encode=\"ordinal\", strategy='uniform')\n",
    "        \n",
    "    def fit_transform(self, observations, actions, rewards, rewardstg):\n",
    "        self.dif = np.array([i*vocab for i in range(st_dims)])\n",
    "        sim_ob_tok = pd.DataFrame(self.ob_enc.fit_transform(observations.values))\n",
    "        sim_tok = pd.concat([sim_ob_tok, actions, rewards, rewardstg], axis=1)\n",
    "        dif_ob_tok = sim_ob_tok.copy()\n",
    "        for i, c in enumerate(dif_ob_tok):\n",
    "            dif_ob_tok[c] = sim_ob_tok[c] + self.dif[i]\n",
    "        dif_tok = pd.concat([dif_ob_tok, actions, rewards, rewardstg], axis=1)\n",
    "        self.total_vocab = dif_tok.max().max()\n",
    "        return dif_tok.values.reshape(-1).astype(int), sim_tok.values.reshape(-1).astype(int)\n",
    "        \n",
    "    def discretize_observation(self, obs):\n",
    "        return self.ob_enc.transform(obs.reshape(1, -1)).reshape(-1)\n",
    "        \n",
    "    def discretize_trajectory(self, traj):\n",
    "        obs = traj[:st_dims]\n",
    "        rest = traj[st_dims:]\n",
    "        print(obs, rest)\n",
    "        sim_obs_tok = self.discretize_observation(obs)\n",
    "        dif_obs_tok = sim_obs_tok + self.dif\n",
    "        sim_tok = np.concatenate([sim_obs_tok, rest])\n",
    "        dif_tok = np.concatenate([dif_obs_tok, rest])\n",
    "        return sim_tok.astype(int), dif_tok.astype(int)\n",
    "    \n",
    "    def similar_to_different(self, tok, mod):\n",
    "        mod = mod%self.traj_len\n",
    "        if mod < st_dims:\n",
    "            return tok\n",
    "        else:\n",
    "            return tok+self.dif[mod]\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.total_vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1db893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 57, 153, 242, ...,   1,   1,   0]),\n",
       " array([57, 53, 42, ...,  1,  1,  0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = DataDiscretizer(vocab ,traj_len)\n",
    "dd.fit_transform(observations, actions, rewards, rewardstg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626f4bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st_cho = int(dd.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e30a66",
   "metadata": {
    "id": "f0e30a66"
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    ix = torch.randint(len(discretized) - block_size, (batch_size,))\n",
    "    x = torch.stack([dif[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([sim[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y, ix%traj_len\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    tt.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        x, y = get_batch()\n",
    "        logits, loss = tt(x, targets=y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    tt.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890143c1",
   "metadata": {
    "id": "890143c1"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(3*block_size, 3*block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8208ce49",
   "metadata": {
    "id": "8208ce49"
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.st_emb = nn.Embedding(st_cho, n_embd)\n",
    "        self.ac_emb = nn.Embedding(ac_cho, n_embd)\n",
    "        self.rw_emb = nn.Embedding(rw_cho, n_embd)\n",
    "        self.rtg_emb= nn.Embedding(rtg_cho, n_embd)\n",
    "        \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        \n",
    "        self.st_head = nn.Linear(n_embd, st_cho)\n",
    "        self.ac_head = nn.Linear(n_embd, ac_cho)\n",
    "        self.rw_head = nn.Linear(n_embd, rw_cho)\n",
    "        self.rtg_head = nn.Linear(n_embd,rtg_cho)\n",
    "\n",
    "    def forward(self, sequence, targets=None):\n",
    "        vocab_embedding = self.embedding(sequence)#(B,T,C)\n",
    "        B, T, C = vocab_embedding.shape\n",
    "        pos_encoding = self.position_embedding_table(torch.arange(T, device=device)) #(T,C)\n",
    "        x = vocab_embedding + pos_encoding #(B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        states, actions, rewards, rewardstg = self.vocabbing(x) \n",
    "\n",
    "        preds = self.lm_head(x) # (B,T,vocab)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = preds.shape\n",
    "            preds = preds.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(preds, targets)\n",
    "\n",
    "        return preds, loss\n",
    "    \n",
    "    def generate_masks(self, mods):\n",
    "        st_mask = torch.stack([(torch.arange(block_size) + mod) % 7 < 4 for mod in mods])\n",
    "        ac_mask = torch.stack([(torch.arange(block_size) + mod) % 7 == 4 for mod in mods])\n",
    "        rw_mask = torch.stack([(torch.arange(block_size) + mod) % 7 == 5 for mod in mods])\n",
    "        rtg_mask = torch.stack([(torch.arange(block_size) + mod)% 7 == 6 for mod in mods])\n",
    "        return st_mask, ac_mask, rw_mask, rtg_mask\n",
    "\n",
    "        \n",
    "    def embedding(self, sequences, mods):\n",
    "        st_mask, ac_mask, rw_mask, rtg_mask = self.generate_masks(mods)\n",
    "        inp = torch.ones([batch_size, block_size, n_embd])\n",
    "        inp[st_mask] = self.st_emb(sequences[st_mask])\n",
    "        inp[ac_mask] = self.ac_emb(sequences[ac_mask])\n",
    "        inp[rw_mask] = self.rw_emb(sequences[rw_mask])\n",
    "        inp[rtg_mask] = self.rtg_emb(sequences[rtg_mask])\n",
    "        ret = inp.reshape([batch_size, block_size, n_embd])\n",
    "        return ret\n",
    "    \n",
    "    def vocabbing(self, out, mods):\n",
    "        st_mask, ac_mask, rw_mask, rtg_mask = self.generate_masks(mods+1)\n",
    "        states = self.st_head(out[st_mask]).reshape(batch_size, -1, st_cho)\n",
    "        actions = self.ac_head(out[ac_mask]).reshape(batch_size, -1, ac_cho)\n",
    "        rewards = self.rw_head(out[rw_mask]).reshape(batch_size, -1, rw_cho)\n",
    "        rewardstg = self.rtg_head(out[rtg_mask]).reshape(batch_size, -1, rtg_cho)\n",
    "        return states, actions, rewards, rewardstg\n",
    "        \n",
    "    def loss(self, states, actions, rewards, rewardstg, targets):\n",
    "        loss = 0\n",
    "        st_mask, ac_mask, rw_mask, rtg_mask = self.generate_masks(mods)\n",
    "        loss += F.cross_entropy(targets[st_mask], states)\n",
    "        loss += F.cross_entropy(targets[ac_mask], actions)\n",
    "        loss += F.cross_entropy(targets[rw_mask], rewards)\n",
    "        loss += F.cross_entropy(targets[rtg_mask], rewardstg)\n",
    "        return loss\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc1afdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf801fc",
   "metadata": {
    "id": "bcf801fc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt = TrajectoryTransformer()\n",
    "tt.to(device)\n",
    "optimizer = torch.optim.AdamW(tt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934a9c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.34594594594594"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11904/(185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcabb811",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'discretized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x, y, mod \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m evaluate \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m.\u001b[39membedding(x, mod)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdiscretized\u001b[49m) \u001b[38;5;241m-\u001b[39m block_size, (batch_size,))\n\u001b[1;32m      3\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([dif[i:i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m      4\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([sim[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'discretized' is not defined"
     ]
    }
   ],
   "source": [
    "x, y, mod = get_batch()\n",
    "evaluate = tt.embedding(x, mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634054e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4634054e",
    "outputId": "9e0c0449-8144-4b10-85b6-2b2347227496"
   },
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    for iteration in range(5000):\n",
    "      # every once in a while evaluate the loss on train and val sets\n",
    "        if iteration % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iteration}: train loss {losses:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        x, y = get_batch()\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = tt(x, targets=y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eQhBWU1orDo1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "eQhBWU1orDo1",
    "outputId": "e3a8e244-81d1-4945-ed50-9e897ec5fcc9"
   },
   "outputs": [],
   "source": [
    "save_path = 'tt_instance_1.model'\n",
    "\n",
    "# Save the model's state dictionary to the file\n",
    "torch.save(tt.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "msf23o-hrbzC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "msf23o-hrbzC",
    "outputId": "42bd7b40-debc-404a-889a-8a5cbda6eaec"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(save_path)\n",
    "tt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3170710",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_length = st_dims + ac_dims + 2*rw_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nVXFx1cDsGke",
   "metadata": {
    "id": "nVXFx1cDsGke"
   },
   "outputs": [],
   "source": [
    "context = discretized.view(-1, trajectory_length)[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs = prevs.astype(int)\n",
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6588b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_discretized(tk, i):\n",
    "    return tk + prevs[i]\n",
    "\n",
    "def trajectory_to_token(traj):\n",
    "    return traj-prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jRzYNg9m3cRk",
   "metadata": {
    "id": "jRzYNg9m3cRk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def beam_search(model, context, st_width=3, ac_width=4):\n",
    "    if type(context) != list and type(context) != np.ndarray:\n",
    "        context = context.detach().cpu().numpy()\n",
    "    window_by_index = [st_width] * st_dims + [ac_width] * ac_dims + [1] + [1]\n",
    "    top_beams_context = [context]\n",
    "    top_beams_prob = [0]\n",
    "\n",
    "    for i in range(trajectory_length):\n",
    "        square_beams_context = []\n",
    "        square_beams_prob = []\n",
    "        k = window_by_index[i]\n",
    "\n",
    "        for seq, prob in zip(top_beams_context, top_beams_prob):\n",
    "            logits, _ = model(torch.tensor(seq[-block_size:], device=device).unsqueeze(0))\n",
    "            logits = F.softmax(logits[0, -1, :], dim=0).detach().cpu().numpy()\n",
    "\n",
    "            top_k = np.argpartition(logits, -k)[-k:]\n",
    "            square_beams_prob.extend(logits[top_k] + prob)\n",
    "\n",
    "            top_dis_k = [token_to_discretized(tk, i) for tk in top_k]\n",
    "            square_beams_context.extend([np.concatenate([seq, tk.reshape(1)]) \\\n",
    "                                  for tk in top_dis_k])\n",
    "\n",
    "        if i < st_dims:\n",
    "            idxs = np.argsort(square_beams_prob)[-k:]\n",
    "        else:\n",
    "            idxs = np.argsort(square_beams_prob)\n",
    "\n",
    "        top_beams_context = np.array(square_beams_context)[idxs]\n",
    "        top_beams_prob = np.array(square_beams_prob)[idxs]\n",
    "    \n",
    "    best_traj = None\n",
    "    traj_rwds = []\n",
    "    \n",
    "    for c in top_beams_context:\n",
    "        traj = c[-trajectory_length:]\n",
    "        traj = trajectory_to_token(traj)\n",
    "        traj_rwd = kbd_rewardstg.inverse_transform((traj[-1]).reshape(-1,1)).reshape(-1)[0]\n",
    "        traj_rwds.append(traj_rwd)\n",
    "    \n",
    "    rwds_idx = np.argsort(traj_rwds)\n",
    "    top_beams_context = top_beams_context[rwds_idx]\n",
    "    top_beams_prob = top_beams_prob[rwds_idx]\n",
    "\n",
    "    return top_beams_context, top_beams_prob\n",
    "\n",
    "def extract_action(trajectory):\n",
    "    return trajectory[st_dims::trajectory_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized.reshape(-1,trajectory_length)[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-fKMpJSysfAQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "-fKMpJSysfAQ",
    "outputId": "d5088ec9-f78f-4fa5-ab95-33fff13bbfd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(context)\n",
    "top_beams_context,  top_beams_prob = beam_search(tt, context, 3, 4)\n",
    "print(top_beams_context.shape, top_beams_prob.shape)\n",
    "# temp = top_beams_context[-1]\n",
    "# print(temp)\n",
    "# print(sorted(rwds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddde2c",
   "metadata": {
    "id": "b6ddde2c"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965775d1",
   "metadata": {
    "id": "965775d1"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(traj):\n",
    "    observations = kbd_observations.transform(np.array(traj[0:4]).reshape(1, -1)).reshape(-1)\n",
    "    action = np.array(traj[4]).reshape(-1)\n",
    "    reward = np.array(traj[5]).reshape(-1)\n",
    "    rewardtg = kbd_rewardstg.transform(np.array(traj[6]).reshape(1, -1)).reshape(-1)\n",
    "    tokenized = np.concatenate([observations, action, reward, rewardtg])\n",
    "    tokenized += prevs \n",
    "    return tokenized.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.array([0.001888, 0.033522, -0.041096, -0.040241, 1.0, 1.0, 21.0])\n",
    "to_tokens(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778261b",
   "metadata": {
    "id": "a778261b"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "def testing(model, context):\n",
    "    total_reward = 0.0\n",
    "    desired_reward = 200\n",
    "\n",
    "    _, _ = env.reset(seed=None)\n",
    "    action = 0\n",
    "    state, reward, terminated, _, _ = env.step(action)\n",
    "    terminated = False\n",
    "    time = 0\n",
    "    \n",
    "    state_ = state.reshape(-1)\n",
    "    action_ = np.array(action).reshape(-1)\n",
    "    reward_ = np.array(reward).reshape(-1)\n",
    "    desired_reward_ = np.array(desired_reward).reshape(-1)\n",
    "\n",
    "    to_encode = np.concatenate([state_, action_, reward_, desired_reward_])\n",
    "    context = to_tokens(to_encode) \n",
    "    print(context)\n",
    "    \n",
    "    while not terminated and time <= 500:\n",
    "        seqs, probs = beam_search(tt, context)\n",
    "#         print(seqs)\n",
    "        action = -2\n",
    "        seq = 'a'\n",
    "        for f in range(11, 0, -1):\n",
    "            seq = seqs[f][-trajectory_length:]\n",
    "            seq = seq - prevs\n",
    "            action = int(seq[4])\n",
    "            if (action == 0 or action == 1):\n",
    "                break\n",
    "#         print(seq+prevs)\n",
    "#         print(action)\n",
    "        \n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        desired_reward -= reward\n",
    "\n",
    "        state_ = state.reshape(-1)\n",
    "        action_ = np.array(action).reshape(-1)\n",
    "        reward_ = np.array(reward).reshape(-1)\n",
    "        desired_reward_ = np.array(desired_reward).reshape(-1)\n",
    "\n",
    "        to_encode = np.concatenate([state_, action_, reward_, desired_reward_])\n",
    "        context = to_tokens(to_encode) \n",
    "        \n",
    "        time += 1\n",
    "#         if time%1 == 0:\n",
    "#             print(time)\n",
    "\n",
    "      # Print reward\n",
    "    print(\"total_reward = {}\".format(total_reward))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5146cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "de5146cf",
    "outputId": "5e12ab43-20c1-49c4-eba3-f274d74ae709",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing(tt, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c5cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f525a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38be22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "\n",
    "test[1:3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10555083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "\n",
    "result = input_array[np.arange(0, len(input_array), 7)].reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = 0\n",
    "st_mask = [(x+mod)%7 < 4 for x in range(11)]\n",
    "ac_mask = [(x+mod)%7 == 4 for x in range(33)]\n",
    "rw_mask = [(x+mod)%7 == 5 for x in range(33)]\n",
    "rtg_mask= [(x+mod)%7 == 6 for x in range(33)]\n",
    "\n",
    "test = np.array(range(11))\n",
    "print(test[st_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3085a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll = nn.Linear(5, 2)\n",
    "inp = torch.tensor(np.random.rand(11, 5), dtype=torch.float)\n",
    "out = ll(inp)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "out[st_mask] = torch.zeros(st_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a26ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373168c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mods = np.array([0, 1, 2])  # Replace with your desired tensor of mods\n",
    "\n",
    "length = 14  # Length of the masks\n",
    "\n",
    "st_mask = torch.stack([(torch.arange(length) + mod) % 7 < 4 for mod in mods])\n",
    "ac_mask = torch.stack([(torch.arange(length) + mod) % 7 == 4 for mod in mods])\n",
    "rw_mask = torch.stack([(torch.arange(length) + mod) % 7 == 5 for mod in mods])\n",
    "rtg_mask = torch.stack([(torch.arange(length) + mod) % 7 == 6 for mod in mods])\n",
    "\n",
    "test = torch.tensor(np.random.rand(3, 14, 5), dtype=torch.float)\n",
    "# print(test)\n",
    "print(test.shape, st_mask.shape)\n",
    "# print(test[rtg_mask].reshape(3, -1, 5))\n",
    "# print(test[rtg_mask].shape)\n",
    "\n",
    "test[rtg_mask] = torch.zeros((6, 5), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47272442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[st_mask] = torch.zeros(st_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9a246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trajectory",
   "language": "python",
   "name": "trajectory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

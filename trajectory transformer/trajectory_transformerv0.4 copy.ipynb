{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e7a37a",
   "metadata": {
    "id": "72e7a37a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guevara/opt/anaconda3/envs/trajectory/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import bisect\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 \n",
    "block_size = 32 \n",
    "max_iters = 5000\n",
    "epochs = 5\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "st_dims = 4\n",
    "ac_dims = 1\n",
    "rw_dims = 1\n",
    "rtg_dims= 1\n",
    "\n",
    "traj_len = 7\n",
    "\n",
    "rw_cho = 1+1\n",
    "ac_cho = 2\n",
    "rtg_cho = 150+1\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "out_vocab = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408a7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cartpole_play.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed583b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>obs1</th>\n",
       "      <th>obs2</th>\n",
       "      <th>obs3</th>\n",
       "      <th>obs4</th>\n",
       "      <th>actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewardstg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048681</td>\n",
       "      <td>0.233331</td>\n",
       "      <td>-0.044224</td>\n",
       "      <td>-0.267353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053347</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>-0.049571</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054124</td>\n",
       "      <td>-0.155511</td>\n",
       "      <td>-0.049350</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051014</td>\n",
       "      <td>-0.349895</td>\n",
       "      <td>-0.043596</td>\n",
       "      <td>0.564419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044016</td>\n",
       "      <td>-0.154190</td>\n",
       "      <td>-0.032307</td>\n",
       "      <td>0.258327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36762</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36764</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36765</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36766</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>-0.229412</td>\n",
       "      <td>-1.206810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36767 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode      obs1      obs2      obs3      obs4  actions  rewards  \\\n",
       "0          0.0  0.048681  0.233331 -0.044224 -0.267353      1.0      1.0   \n",
       "1          0.0  0.053347  0.038867 -0.049571  0.011060      0.0      1.0   \n",
       "2          0.0  0.054124 -0.155511 -0.049350  0.287700      0.0      1.0   \n",
       "3          0.0  0.051014 -0.349895 -0.043596  0.564419      0.0      1.0   \n",
       "4          0.0  0.044016 -0.154190 -0.032307  0.258327      1.0      1.0   \n",
       "...        ...       ...       ...       ...       ...      ...      ...   \n",
       "36762   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36763   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "36764   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36765   1099.0  0.145006  0.416619 -0.229412 -1.206810      0.0      1.0   \n",
       "36766   1099.0  0.145006  0.416619 -0.229412 -1.206810      1.0      1.0   \n",
       "\n",
       "       rewardstg  \n",
       "0           66.0  \n",
       "1           65.0  \n",
       "2           64.0  \n",
       "3           63.0  \n",
       "4           62.0  \n",
       "...          ...  \n",
       "36762        4.0  \n",
       "36763        3.0  \n",
       "36764        2.0  \n",
       "36765        1.0  \n",
       "36766       -0.0  \n",
       "\n",
       "[36767 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rename = {'0':'episode', '1':'obs1', '2':'obs2', '3':'obs3', '4':'obs4', '5':'actions', '6':'rewards', '7':'rewardstg'}\n",
    "data = data.rename(rename, axis=1)\n",
    "data.drop(columns='Unnamed: 0', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0545c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = data[['obs1', 'obs2', 'obs3', 'obs4']]\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']-1\n",
    "rewardstg = data['rewardstg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ad3cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.],\n",
       "       [43.],\n",
       "       [42.],\n",
       "       ...,\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = KBinsDiscretizer(100, encode=\"ordinal\", strategy='uniform')\n",
    "b = kb.fit_transform(np.array(rewardstg).reshape(-1, 1))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77eeb093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        66.0\n",
       "1        65.0\n",
       "2        64.0\n",
       "3        63.0\n",
       "4        62.0\n",
       "         ... \n",
       "36762     4.0\n",
       "36763     3.0\n",
       "36764     2.0\n",
       "36765     1.0\n",
       "36766    -0.0\n",
       "Name: rewardstg, Length: 36767, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardstg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ee3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class will work with numpy arrays as inputs, except fit_transform\n",
    "class DataDiscretizer:\n",
    "    def __init__(self, vocab, traj_len, encode='ordinal', strategy='uniform'):\n",
    "        self.vocab = vocab\n",
    "        self.traj_len = traj_len\n",
    "        self.ob_enc = KBinsDiscretizer(vocab, encode=\"ordinal\", strategy='uniform')\n",
    "        self.rtg_enc = KBinsDiscretizer(vocab, encode=\"ordinal\", strategy=\"uniform\")\n",
    "        \n",
    "    def fit_transform(self, observations, actions, rewards, rewardstg):\n",
    "        self.dif = np.array([i*out_vocab for i in range(traj_len)])\n",
    "        sim_ob_tok = pd.DataFrame(self.ob_enc.fit_transform(observations.values))\n",
    "        sim_rtg_tok = pd.DataFrame(self.rtg_enc.fit_transform(rewardstg.values.reshape(-1, 1)))\n",
    "        sim_tok = pd.concat([sim_ob_tok, actions, rewards, sim_rtg_tok], axis=1)\n",
    "        dif_tok = sim_tok.copy() + self.dif\n",
    "        self.total_vocab = dif_tok.max().max()\n",
    "        return dif_tok.values.reshape(-1).astype(int), sim_tok.values.reshape(-1).astype(int)\n",
    "        \n",
    "    def discretize_observation(self, obs):\n",
    "        return self.ob_enc.transform(obs.reshape(1, -1)).reshape(-1)\n",
    "    \n",
    "    def discretize_rtg(self, rtg):\n",
    "        return self.rtg_enc.transform(rtg.reshape(1, -1)).reshape(-1)\n",
    "        \n",
    "    def discretize_trajectory(self, traj):\n",
    "        obs = traj[:st_dims]\n",
    "        anr = traj[st_dims:-1]\n",
    "        rtg = traj[-1:]\n",
    "        print(obs, anr, rtg)\n",
    "        sim_obs_tok = self.discretize_observation(obs)\n",
    "        sim_rtg_tok = self.discretize_rtg(rtg)\n",
    "        sim_tok = np.concatenate([sim_obs_tok, anr, sim_rtg_tok])\n",
    "        dif_tok = sim_tok.copy() + self.dif\n",
    "        return sim_tok.astype(int), dif_tok.astype(int)\n",
    "    \n",
    "    def similar_to_different(self, tok, mod):\n",
    "        mod = mod%self.traj_len\n",
    "        return tok+self.dif[mod]\n",
    "    \n",
    "    def get_raw(self, tok, mod)\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.total_vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1db893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DataDiscretizer(100, 7)\n",
    "dif, sim = dd.fit_transform(observations, actions, rewards, rewardstg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d2c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab = int(dd.get_vocab_size())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f019c812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 57, 153, 242, ..., 401, 500, 644],\n",
       "       [ 57, 149, 241, ..., 400, 500, 643],\n",
       "       [ 57, 146, 241, ..., 400, 500, 642],\n",
       "       ...,\n",
       "       [ 63, 156, 206, ..., 400, 500, 601],\n",
       "       [ 63, 156, 206, ..., 400, 500, 600],\n",
       "       [ 63, 156, 206, ..., 401, 500, 600]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif.reshape(-1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2cf9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = torch.tensor(dif, dtype=torch.long)\n",
    "sim = torch.tensor(sim, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e30a66",
   "metadata": {
    "id": "f0e30a66"
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    ix = torch.randint(len(dif) - block_size, (batch_size,))\n",
    "    x = torch.stack([dif[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([sim[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y, ix%traj_len\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    tt.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        x, y, mod = get_batch()\n",
    "        logits, loss = tt(x, mod, targets=y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    tt.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890143c1",
   "metadata": {
    "id": "890143c1"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(3*block_size, 3*block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f3aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [100, 100, 100, 100, 2, 1, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8208ce49",
   "metadata": {
    "id": "8208ce49"
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inp_embedding = nn.Embedding(inp_vocab, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, out_vocab)\n",
    "\n",
    "    def forward(self, sequence, mods, targets=None):\n",
    "        vocab_embedding = self.inp_embedding(sequence) #(B,T,C)\n",
    "        B, T, C = vocab_embedding.shape\n",
    "        pos_encoding = self.position_embedding_table(torch.arange(T, device=device)) #(T,C)\n",
    "        x = vocab_embedding + pos_encoding #(B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) \n",
    "\n",
    "        preds = self.lm_head(x) # (B,T,100)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = preds.shape\n",
    "            mask = self.generate_mask(mods)\n",
    "            preds = preds.masked_fill(mask, float('-inf'))\n",
    "            preds = preds.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(preds, targets)\n",
    "\n",
    "        return preds, loss\n",
    "    \n",
    "    def generate_mask(self, mods):\n",
    "        mask = torch.stack(\n",
    "        [torch.stack(\n",
    "            [(torch.arange(out_vocab)) >= pattern[(i+mod+1)%traj_len]\n",
    "                 for i in range(block_size)]) \n",
    "                     for mod in mods])\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c81e708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TrajectoryTransformer()\n",
    "tt.to(device)\n",
    "optimizer = torch.optim.AdamW(tt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634054e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4634054e",
    "outputId": "9e0c0449-8144-4b10-85b6-2b2347227496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.5802\n",
      "step 100: train loss 2.0173\n",
      "step 200: train loss 1.9878\n",
      "step 300: train loss 1.8130\n",
      "step 400: train loss 1.6860\n",
      "step 500: train loss 1.5572\n",
      "step 600: train loss 1.4696\n",
      "step 700: train loss 1.3461\n",
      "step 800: train loss 1.2822\n",
      "step 900: train loss 1.2242\n",
      "step 1000: train loss 1.1747\n",
      "step 1100: train loss 1.1051\n",
      "step 1200: train loss 1.0718\n",
      "step 1300: train loss 1.0194\n",
      "step 1400: train loss 0.9679\n",
      "step 1500: train loss 0.9559\n",
      "step 1600: train loss 0.9469\n",
      "step 1700: train loss 0.9154\n",
      "step 1800: train loss 0.8994\n",
      "step 1900: train loss 0.8564\n",
      "step 2000: train loss 0.8538\n",
      "step 2100: train loss 0.8376\n",
      "step 2200: train loss 0.8033\n",
      "step 2300: train loss 0.8295\n",
      "step 2400: train loss 0.8114\n",
      "step 2500: train loss 0.7914\n",
      "step 2600: train loss 0.7615\n",
      "step 2700: train loss 0.7744\n",
      "step 2800: train loss 0.7654\n",
      "step 2900: train loss 0.7738\n",
      "step 3000: train loss 0.7399\n",
      "step 3100: train loss 0.7246\n",
      "step 3200: train loss 0.7343\n",
      "step 3300: train loss 0.7250\n",
      "step 3400: train loss 0.7068\n",
      "step 3500: train loss 0.7109\n",
      "step 3600: train loss 0.6987\n",
      "step 3700: train loss 0.6900\n",
      "step 3800: train loss 0.6804\n",
      "step 3900: train loss 0.6930\n",
      "step 4000: train loss 0.6910\n",
      "step 4100: train loss 0.6690\n",
      "step 4200: train loss 0.6676\n",
      "step 4300: train loss 0.6867\n",
      "step 4400: train loss 0.6675\n",
      "step 4500: train loss 0.6771\n",
      "step 4600: train loss 0.6568\n",
      "step 4700: train loss 0.6619\n",
      "step 4800: train loss 0.6607\n",
      "step 4900: train loss 0.6626\n",
      "step 0: train loss 0.6517\n",
      "step 100: train loss 0.6447\n",
      "step 200: train loss 0.6545\n",
      "step 300: train loss 0.6602\n",
      "step 400: train loss 0.6561\n",
      "step 500: train loss 0.6493\n",
      "step 600: train loss 0.6309\n",
      "step 700: train loss 0.6338\n",
      "step 800: train loss 0.6482\n",
      "step 900: train loss 0.6325\n",
      "step 1000: train loss 0.6172\n",
      "step 1100: train loss 0.6282\n",
      "step 1200: train loss 0.6145\n",
      "step 1300: train loss 0.6210\n",
      "step 1400: train loss 0.6320\n",
      "step 1500: train loss 0.6240\n",
      "step 1600: train loss 0.6164\n",
      "step 1700: train loss 0.6073\n",
      "step 1800: train loss 0.6216\n",
      "step 1900: train loss 0.5972\n",
      "step 2000: train loss 0.6151\n",
      "step 2100: train loss 0.6064\n",
      "step 2200: train loss 0.6165\n",
      "step 2300: train loss 0.5951\n",
      "step 2400: train loss 0.6072\n",
      "step 2500: train loss 0.5874\n",
      "step 2600: train loss 0.6049\n",
      "step 2700: train loss 0.6036\n",
      "step 2800: train loss 0.5856\n",
      "step 2900: train loss 0.6043\n",
      "step 3000: train loss 0.5949\n",
      "step 3100: train loss 0.5975\n",
      "step 3200: train loss 0.5929\n",
      "step 3300: train loss 0.5924\n",
      "step 3400: train loss 0.6152\n",
      "step 3500: train loss 0.5924\n",
      "step 3600: train loss 0.5854\n",
      "step 3700: train loss 0.5912\n",
      "step 3800: train loss 0.5893\n",
      "step 3900: train loss 0.5824\n",
      "step 4000: train loss 0.5896\n",
      "step 4100: train loss 0.5830\n",
      "step 4200: train loss 0.5860\n",
      "step 4300: train loss 0.5871\n",
      "step 4400: train loss 0.5849\n",
      "step 4500: train loss 0.5719\n",
      "step 4600: train loss 0.5871\n",
      "step 4700: train loss 0.5907\n",
      "step 4800: train loss 0.5802\n",
      "step 4900: train loss 0.5806\n",
      "step 0: train loss 0.5778\n",
      "step 100: train loss 0.5831\n",
      "step 200: train loss 0.5805\n",
      "step 300: train loss 0.5859\n",
      "step 400: train loss 0.5703\n",
      "step 500: train loss 0.5863\n",
      "step 600: train loss 0.5930\n",
      "step 700: train loss 0.5660\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    for iteration in range(5000):\n",
    "      # every once in a while evaluate the loss on train and val sets\n",
    "        if iteration % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iteration}: train loss {losses:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        x, y, mods = get_batch()\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = tt(x, mods,targets=y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44bd63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TrajectoryTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eQhBWU1orDo1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "eQhBWU1orDo1",
    "outputId": "e3a8e244-81d1-4945-ed50-9e897ec5fcc9"
   },
   "outputs": [],
   "source": [
    "save_path = 'tt_instance_1.model'\n",
    "\n",
    "# Save the model's state dictionary to the file\n",
    "torch.save(tt.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "msf23o-hrbzC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "msf23o-hrbzC",
    "outputId": "42bd7b40-debc-404a-889a-8a5cbda6eaec"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(save_path)\n",
    "tt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3170710",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_length = st_dims + ac_dims + 2*rw_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nVXFx1cDsGke",
   "metadata": {
    "id": "nVXFx1cDsGke"
   },
   "outputs": [],
   "source": [
    "context = discretized.view(-1, trajectory_length)[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs = prevs.astype(int)\n",
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6588b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_discretized(tk, i):\n",
    "    return tk + prevs[i]\n",
    "\n",
    "def trajectory_to_token(traj):\n",
    "    return traj-prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jRzYNg9m3cRk",
   "metadata": {
    "id": "jRzYNg9m3cRk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def beam_search(model, context, st_width=3, ac_width=4):\n",
    "    if type(context) != list and type(context) != np.ndarray:\n",
    "        context = context.detach().cpu().numpy()\n",
    "    window_by_index = [st_width] * st_dims + [ac_width] * ac_dims + [1] + [1]\n",
    "    top_beams_context = [context]\n",
    "    top_beams_prob = [0]\n",
    "\n",
    "    for i in range(trajectory_length):\n",
    "        square_beams_context = []\n",
    "        square_beams_prob = []\n",
    "        k = window_by_index[i]\n",
    "\n",
    "        for seq, prob in zip(top_beams_context, top_beams_prob):\n",
    "            logits, _ = model(torch.tensor(seq[-block_size:], device=device).unsqueeze(0))\n",
    "            logits = F.softmax(logits[0, -1, :], dim=0).detach().cpu().numpy()\n",
    "\n",
    "            top_k = np.argpartition(logits, -k)[-k:]\n",
    "            square_beams_prob.extend(logits[top_k] + prob)\n",
    "\n",
    "            top_dis_k = [token_to_discretized(tk, i) for tk in top_k]\n",
    "            square_beams_context.extend([np.concatenate([seq, tk.reshape(1)]) \\\n",
    "                                  for tk in top_dis_k])\n",
    "\n",
    "        if i < st_dims:\n",
    "            idxs = np.argsort(square_beams_prob)[-k:]\n",
    "        else:\n",
    "            idxs = np.argsort(square_beams_prob)\n",
    "\n",
    "        top_beams_context = np.array(square_beams_context)[idxs]\n",
    "        top_beams_prob = np.array(square_beams_prob)[idxs]\n",
    "    \n",
    "    best_traj = None\n",
    "    traj_rwds = []\n",
    "    \n",
    "    for c in top_beams_context:\n",
    "        traj = c[-trajectory_length:]\n",
    "        traj = trajectory_to_token(traj)\n",
    "        traj_rwd = kbd_rewardstg.inverse_transform((traj[-1]).reshape(-1,1)).reshape(-1)[0]\n",
    "        traj_rwds.append(traj_rwd)\n",
    "    \n",
    "    rwds_idx = np.argsort(traj_rwds)\n",
    "    top_beams_context = top_beams_context[rwds_idx]\n",
    "    top_beams_prob = top_beams_prob[rwds_idx]\n",
    "\n",
    "    return top_beams_context, top_beams_prob\n",
    "\n",
    "def extract_action(trajectory):\n",
    "    return trajectory[st_dims::trajectory_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized.reshape(-1,trajectory_length)[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-fKMpJSysfAQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "-fKMpJSysfAQ",
    "outputId": "d5088ec9-f78f-4fa5-ab95-33fff13bbfd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(context)\n",
    "top_beams_context,  top_beams_prob = beam_search(tt, context, 3, 4)\n",
    "print(top_beams_context.shape, top_beams_prob.shape)\n",
    "# temp = top_beams_context[-1]\n",
    "# print(temp)\n",
    "# print(sorted(rwds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddde2c",
   "metadata": {
    "id": "b6ddde2c"
   },
   "source": [
    "# Experimenting maybe we need it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965775d1",
   "metadata": {
    "id": "965775d1"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(traj):\n",
    "    observations = kbd_observations.transform(np.array(traj[0:4]).reshape(1, -1)).reshape(-1)\n",
    "    action = np.array(traj[4]).reshape(-1)\n",
    "    reward = np.array(traj[5]).reshape(-1)\n",
    "    rewardtg = kbd_rewardstg.transform(np.array(traj[6]).reshape(1, -1)).reshape(-1)\n",
    "    tokenized = np.concatenate([observations, action, reward, rewardtg])\n",
    "    tokenized += prevs \n",
    "    return tokenized.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.array([0.001888, 0.033522, -0.041096, -0.040241, 1.0, 1.0, 21.0])\n",
    "to_tokens(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778261b",
   "metadata": {
    "id": "a778261b"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "def testing(model, context):\n",
    "    total_reward = 0.0\n",
    "    desired_reward = 200\n",
    "\n",
    "    _, _ = env.reset(seed=None)\n",
    "    action = 0\n",
    "    state, reward, terminated, _, _ = env.step(action)\n",
    "    terminated = False\n",
    "    time = 0\n",
    "    \n",
    "    state_ = state.reshape(-1)\n",
    "    action_ = np.array(action).reshape(-1)\n",
    "    reward_ = np.array(reward).reshape(-1)\n",
    "    desired_reward_ = np.array(desired_reward).reshape(-1)\n",
    "\n",
    "    to_encode = np.concatenate([state_, action_, reward_, desired_reward_])\n",
    "    context = to_tokens(to_encode) \n",
    "    print(context)\n",
    "    \n",
    "    while not terminated and time <= 500:\n",
    "        seqs, probs = beam_search(tt, context)\n",
    "#         print(seqs)\n",
    "        action = -2\n",
    "        seq = 'a'\n",
    "        for f in range(11, 0, -1):\n",
    "            seq = seqs[f][-trajectory_length:]\n",
    "            seq = seq - prevs\n",
    "            action = int(seq[4])\n",
    "            if (action == 0 or action == 1):\n",
    "                break\n",
    "#         print(seq+prevs)\n",
    "#         print(action)\n",
    "        \n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        desired_reward -= reward\n",
    "\n",
    "        state_ = state.reshape(-1)\n",
    "        action_ = np.array(action).reshape(-1)\n",
    "        reward_ = np.array(reward).reshape(-1)\n",
    "        desired_reward_ = np.array(desired_reward).reshape(-1)\n",
    "\n",
    "        to_encode = np.concatenate([state_, action_, reward_, desired_reward_])\n",
    "        context = to_tokens(to_encode) \n",
    "        \n",
    "        time += 1\n",
    "#         if time%1 == 0:\n",
    "#             print(time)\n",
    "\n",
    "      # Print reward\n",
    "    print(\"total_reward = {}\".format(total_reward))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5146cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "de5146cf",
    "outputId": "5e12ab43-20c1-49c4-eba3-f274d74ae709",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing(tt, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c5cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f525a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38be22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "\n",
    "test[1:3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10555083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "\n",
    "result = input_array[np.arange(0, len(input_array), 7)].reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = 0\n",
    "st_mask = [(x+mod)%7 < 4 for x in range(11)]\n",
    "ac_mask = [(x+mod)%7 == 4 for x in range(33)]\n",
    "rw_mask = [(x+mod)%7 == 5 for x in range(33)]\n",
    "rtg_mask= [(x+mod)%7 == 6 for x in range(33)]\n",
    "\n",
    "test = np.array(range(11))\n",
    "print(test[st_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3085a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll = nn.Linear(5, 2)\n",
    "inp = torch.tensor(np.random.rand(11, 5), dtype=torch.float)\n",
    "out = ll(inp)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "out[st_mask] = torch.zeros(st_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a26ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373168c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mods = np.array([0, 1, 2])  # Replace with your desired tensor of mods\n",
    "\n",
    "length = 14  # Length of the masks\n",
    "\n",
    "st_mask = torch.stack([(torch.arange(length) + mod) % 7 < 4 for mod in mods])\n",
    "ac_mask = torch.stack([(torch.arange(length) + mod) % 7 == 4 for mod in mods])\n",
    "rw_mask = torch.stack([(torch.arange(length) + mod) % 7 == 5 for mod in mods])\n",
    "rtg_mask = torch.stack([(torch.arange(length) + mod) % 7 == 6 for mod in mods])\n",
    "\n",
    "test = torch.tensor(np.random.rand(3, 14, 5), dtype=torch.float)\n",
    "# print(test)\n",
    "print(test.shape, st_mask.shape)\n",
    "# print(test[rtg_mask].reshape(3, -1, 5))\n",
    "# print(test[rtg_mask].shape)\n",
    "\n",
    "test[rtg_mask] = torch.zeros((6, 5), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47272442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[st_mask] = torch.zeros(st_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4eff02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabb811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt =TrajectoryTransformer()\n",
    "x, y, mod = get_batch()\n",
    "evaluate = tt.generate_mask(mod)\n",
    "emb = nn.Embedding(1000, 100)(x)\n",
    "print(mod)\n",
    "t = emb.masked_fill(evaluate, float('-inf'))\n",
    "\n",
    "l = F.cross_entropy(t.reshape(-1, 100), y.reshape(-1))\n",
    "print(l)\n",
    "x[0:2], y[0:2]\n",
    "t.reshape(-1, 100)[4]\n",
    "y.reshape(-1)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42089c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trajectory",
   "language": "python",
   "name": "trajectory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

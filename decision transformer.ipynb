{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e7a37a",
      "metadata": {
        "id": "72e7a37a"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "n_features = 1\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 11 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "st_dims = 8\n",
        "ac_dims = 4\n",
        "rw_dims = 1\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdea36ef",
      "metadata": {
        "id": "cdea36ef"
      },
      "outputs": [],
      "source": [
        "cols = [\"Unnamed: 0\", \"X Coordinate\", \"Y Coordinate\", \"Linear Velocity X\", \"Linear Velocity Y\", \"Angle\", \\\n",
        "       \"Angular Velocity\", \"Leg1\", \"Leg2\", \"index\"]\n",
        "renames = {'Action':'actions', 'Reward':'rewards', 'observation_x':'observations', 'observation_y':'next_observations'}\n",
        "\n",
        "def get_data():\n",
        "    dataframe = pd.read_csv(\"medium play.csv\")\n",
        "    dataframe['observation'] = list(dataframe.iloc[:, 1:9].values)\n",
        "    observations = dataframe['observation'].copy()\n",
        "    observations.drop(observations.index[0], inplace=True)\n",
        "    dataframe.drop(dataframe.index[-1], inplace=True)\n",
        "    observations = observations.reset_index()\n",
        "    merged_data = dataframe.merge(observations, left_index=True, right_index=True)\n",
        "    merged_data[\"terminals\"] = False\n",
        "    merged_data.loc[184753:, \"Episode\"] += 2000\n",
        "    grouped_episodes = merged_data.groupby(\"Episode\")\n",
        "    merged_data = merged_data.rename(columns=renames)\n",
        "    for episode, group in grouped_episodes:\n",
        "        last_index = group.index[-1]  # Get the index of the last row in the group\n",
        "        merged_data.loc[last_index, 'terminals'] = True\n",
        "        every = group.index\n",
        "        merged_data.loc[every, 'rewardstg'] = merged_data.loc[every, 'rewards'].cumsum()\n",
        "        merged_data.loc[every, 'rewardstg'] -= merged_data.loc[every[-1], 'rewardstg']\n",
        "        merged_data.loc[every, 'rewardstg'] *= -1\n",
        "    merged_data.drop(cols, inplace=True, axis=1)\n",
        "    observations = np.vstack(merged_data['observations'])\n",
        "    next_observations = np.vstack(merged_data['next_observations'])\n",
        "    actions = torch.tensor(merged_data[\"actions\"].values, dtype=torch.long)\n",
        "    rewards = torch.tensor(merged_data[\"rewardstg\"].values, dtype=torch.float)\n",
        "    observations = torch.tensor(observations, dtype=torch.float)\n",
        "    next_observations = torch.tensor(next_observations)\n",
        "    return merged_data, actions, rewards, observations\n",
        "\n",
        "\n",
        "data, actions, rewards, observations = get_data()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473b09fc",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "473b09fc",
        "outputId": "4e625770-6fe8-4e09-e5d4-615e8c302504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([400093]) torch.Size([400093]) torch.Size([400093, 8])\n"
          ]
        }
      ],
      "source": [
        "print(actions.shape, rewards.shape, observations.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890143c1",
      "metadata": {
        "id": "890143c1"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, ix):\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "def get_intercalated():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    ax, ay = get_batch(actions, ix)\n",
        "    rx, ry = get_batch(rewards, ix)\n",
        "    sx, sy = get_batch(observations, ix)\n",
        "    return ax, ay, rx, ry, sx, sy\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    dt.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        ax, ay, rx, ry, sx, sy = get_intercalated()\n",
        "        logits, loss = dt(sx, ax, rx, targets=ay)\n",
        "        losses[k] = loss.item()\n",
        "    out = losses.mean()\n",
        "    dt.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(3*block_size, 3*block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class DecisionTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.st_embd = nn.Linear(st_dims, n_embd)\n",
        "        self.ac_embd = nn.Embedding(ac_dims, n_embd)\n",
        "        self.rw_embd = nn.Linear(rw_dims, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, ac_dims)\n",
        "\n",
        "    def forward(self, states, actions, rewards, targets=None):\n",
        "        x = self.embeddings(states, actions, rewards)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "\n",
        "        acs = x[:, 2::3, :]\n",
        "        preds = self.lm_head(acs) # (B,T,n_features)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = preds.shape\n",
        "            preds = preds.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(preds, targets)\n",
        "\n",
        "        return preds, loss\n",
        "\n",
        "    def embeddings(self, states, actions, rewards):\n",
        "        states = self.st_embd(states)\n",
        "        actions = self.ac_embd(actions)\n",
        "        rewards = self.rw_embd(rewards.unsqueeze(-1))\n",
        "        B, T, C = states.shape\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        #RSA\n",
        "        intercalated = torch.tensor(np.zeros((B, 3*T, C)), dtype=torch.float, device=device)\n",
        "        intercalated[:, 0::3,:] = rewards + pos_emb\n",
        "        intercalated[:, 1::3, :] = states + pos_emb\n",
        "        intercalated[:, 2::3, :] = actions + pos_emb\n",
        "\n",
        "        return intercalated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c19d081",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c19d081",
        "outputId": "1bb2eea9-303e-46ad-eb9d-48fbbf19b7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 33, 64])\n"
          ]
        }
      ],
      "source": [
        "dt = DecisionTransformer().to(device)\n",
        "ax, ay, rx, ry, sx, sy = get_intercalated()\n",
        "embedding = dt.embeddings(sx, ax, rx)\n",
        "print(embedding.shape)\n",
        "preds, loss = dt.forward(sx, ax, rx, targets=ay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f5b492",
      "metadata": {
        "id": "35f5b492"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(dt.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4634054e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4634054e",
        "outputId": "1c766c1f-56ea-4923-d528-32c2c9fdd869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 0.8205\n",
            "step 100: train loss 0.8196\n",
            "step 200: train loss 0.7926\n",
            "step 300: train loss 0.7870\n",
            "step 400: train loss 0.7928\n",
            "step 500: train loss 0.7801\n",
            "step 600: train loss 0.8071\n",
            "step 700: train loss 0.7891\n",
            "step 800: train loss 0.7900\n",
            "step 900: train loss 0.7846\n",
            "step 1000: train loss 0.8049\n",
            "step 1100: train loss 0.7790\n",
            "step 1200: train loss 0.7872\n",
            "step 1300: train loss 0.7713\n",
            "step 1400: train loss 0.8011\n",
            "step 1500: train loss 0.7864\n",
            "step 1600: train loss 0.7892\n",
            "step 1700: train loss 0.7874\n",
            "step 1800: train loss 0.7510\n",
            "step 1900: train loss 0.7804\n",
            "step 2000: train loss 0.7818\n",
            "step 2100: train loss 0.7733\n",
            "step 2200: train loss 0.7508\n",
            "step 2300: train loss 0.7596\n",
            "step 2400: train loss 0.7719\n",
            "step 2500: train loss 0.7476\n",
            "step 2600: train loss 0.7700\n",
            "step 2700: train loss 0.7648\n",
            "step 2800: train loss 0.7640\n",
            "step 2900: train loss 0.7632\n",
            "step 3000: train loss 0.7673\n",
            "step 3100: train loss 0.7559\n",
            "step 3200: train loss 0.8106\n",
            "step 3300: train loss 0.7699\n",
            "step 3400: train loss 0.7733\n",
            "step 3500: train loss 0.7669\n",
            "step 3600: train loss 0.7591\n",
            "step 3700: train loss 0.7714\n",
            "step 3800: train loss 0.7680\n",
            "step 3900: train loss 0.7716\n",
            "step 4000: train loss 0.7482\n",
            "step 4100: train loss 0.7767\n",
            "step 4200: train loss 0.7673\n",
            "step 4300: train loss 0.7369\n",
            "step 4400: train loss 0.7615\n",
            "step 4500: train loss 0.7536\n",
            "step 4600: train loss 0.7667\n",
            "step 4700: train loss 0.7566\n",
            "step 4800: train loss 0.7616\n",
            "step 4900: train loss 0.7630\n",
            "step 0: train loss 0.7384\n",
            "step 100: train loss 0.7481\n",
            "step 200: train loss 0.7326\n",
            "step 300: train loss 0.7553\n",
            "step 400: train loss 0.7614\n",
            "step 500: train loss 0.7351\n",
            "step 600: train loss 0.7302\n",
            "step 700: train loss 0.7450\n",
            "step 800: train loss 0.7940\n",
            "step 900: train loss 0.7545\n",
            "step 1000: train loss 0.7147\n",
            "step 1100: train loss 0.7419\n",
            "step 1200: train loss 0.7766\n",
            "step 1300: train loss 0.7526\n",
            "step 1400: train loss 0.7437\n",
            "step 1500: train loss 0.7455\n",
            "step 1600: train loss 0.7561\n",
            "step 1700: train loss 0.7621\n",
            "step 1800: train loss 0.7325\n",
            "step 1900: train loss 0.7428\n",
            "step 2000: train loss 0.7274\n",
            "step 2100: train loss 0.7460\n",
            "step 2200: train loss 0.7428\n",
            "step 2300: train loss 0.7330\n",
            "step 2400: train loss 0.7360\n",
            "step 2500: train loss 0.7381\n",
            "step 2600: train loss 0.7459\n",
            "step 2700: train loss 0.7291\n",
            "step 2800: train loss 0.7396\n",
            "step 2900: train loss 0.7596\n",
            "step 3000: train loss 0.7523\n",
            "step 3100: train loss 0.7133\n",
            "step 3200: train loss 0.7397\n",
            "step 3300: train loss 0.7488\n",
            "step 3400: train loss 0.7139\n",
            "step 3500: train loss 0.7321\n",
            "step 3600: train loss 0.7357\n",
            "step 3700: train loss 0.7297\n",
            "step 3800: train loss 0.7486\n",
            "step 3900: train loss 0.7304\n",
            "step 4000: train loss 0.7313\n",
            "step 4100: train loss 0.7195\n",
            "step 4200: train loss 0.7505\n",
            "step 4300: train loss 0.7434\n",
            "step 4400: train loss 0.7327\n",
            "step 4500: train loss 0.7371\n",
            "step 4600: train loss 0.7457\n",
            "step 4700: train loss 0.7296\n",
            "step 4800: train loss 0.7347\n",
            "step 4900: train loss 0.7282\n",
            "step 0: train loss 0.7484\n",
            "step 100: train loss 0.7581\n",
            "step 200: train loss 0.7222\n",
            "step 300: train loss 0.7272\n",
            "step 400: train loss 0.7164\n",
            "step 500: train loss 0.7641\n",
            "step 600: train loss 0.7286\n",
            "step 700: train loss 0.7233\n",
            "step 800: train loss 0.7471\n",
            "step 900: train loss 0.7276\n",
            "step 1000: train loss 0.7412\n",
            "step 1100: train loss 0.7362\n",
            "step 1200: train loss 0.7260\n",
            "step 1300: train loss 0.7297\n",
            "step 1400: train loss 0.7578\n",
            "step 1500: train loss 0.7201\n",
            "step 1600: train loss 0.7053\n",
            "step 1700: train loss 0.7299\n",
            "step 1800: train loss 0.7303\n",
            "step 1900: train loss 0.7384\n",
            "step 2000: train loss 0.7337\n",
            "step 2100: train loss 0.7374\n",
            "step 2200: train loss 0.7438\n",
            "step 2300: train loss 0.7254\n",
            "step 2400: train loss 0.7395\n",
            "step 2500: train loss 0.7344\n",
            "step 2600: train loss 0.7257\n",
            "step 2700: train loss 0.7371\n",
            "step 2800: train loss 0.7254\n",
            "step 2900: train loss 0.7003\n",
            "step 3000: train loss 0.7378\n",
            "step 3100: train loss 0.7414\n",
            "step 3200: train loss 0.7417\n",
            "step 3300: train loss 0.7209\n",
            "step 3400: train loss 0.7140\n",
            "step 3500: train loss 0.7289\n",
            "step 3600: train loss 0.7362\n",
            "step 3700: train loss 0.7266\n",
            "step 3800: train loss 0.7203\n",
            "step 3900: train loss 0.7447\n",
            "step 4000: train loss 0.7240\n",
            "step 4100: train loss 0.7243\n",
            "step 4200: train loss 0.7258\n",
            "step 4300: train loss 0.7320\n",
            "step 4400: train loss 0.7245\n",
            "step 4500: train loss 0.7303\n",
            "step 4600: train loss 0.7432\n",
            "step 4700: train loss 0.7366\n",
            "step 4800: train loss 0.7411\n",
            "step 4900: train loss 0.7155\n",
            "step 0: train loss 0.7525\n",
            "step 100: train loss 0.7013\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ef187ceab977>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0may\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for _ in range(10):\n",
        "  for iteration in range(5000):\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "      if iteration % eval_interval == 0 or iter == max_iters - 1:\n",
        "          losses = estimate_loss()\n",
        "          print(f\"step {iteration}: train loss {losses:.4f}\")\n",
        "\n",
        "      # sample a batch of data\n",
        "      ax, ay, rx, ry, sx, sy = get_intercalated()\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = dt(sx, ax, rx, targets=ay)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lExTypHE4Cio",
        "outputId": "de68ad2a-a9ad-4c8e-cf2b-0e25b512d60c"
      },
      "id": "lExTypHE4Cio",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.6.3)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.22.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.6.3)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.1.3)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4a869b",
      "metadata": {
        "id": "8a4a869b"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b43d7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49b43d7f",
        "outputId": "90b988a5-ab79-4088-e1b2-350761596ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment LunarLander-v2 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_reward = -602.5747525008724\n"
          ]
        }
      ],
      "source": [
        "# Create policy\n",
        "dt.eval()\n",
        "\n",
        "# Create environment\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "for i in range(1):\n",
        "  # Reset it\n",
        "  total_reward = 0.0\n",
        "\n",
        "  #numpy array (C)\n",
        "  state = env.reset(seed=None)\n",
        "\n",
        "  # While the episode is not finished\n",
        "  terminated = False\n",
        "  time = 0\n",
        "\n",
        "  desired = -100\n",
        "  rewards = torch.full((1, rw_dims), desired, dtype=torch.float, device=device)\n",
        "  states = torch.zeros((1, 1, st_dims), dtype=torch.float, device=device)\n",
        "  states[0, 0, :] = torch.tensor(state, dtype=torch.float, device=device)\n",
        "  actions = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  actions[0, 0] = 0.0\n",
        "\n",
        "  # define a trigger function, return True to start recording a new video:\n",
        "  episode_trigger=lambda episode_id: (episode_id%1==0)\n",
        "\n",
        "  # decorate env with a video recorder:\n",
        "  env = gym.wrappers.RecordVideo(env, \"./video\",\n",
        "          episode_trigger=episode_trigger, video_length=100000)\n",
        "\n",
        "  env.start_video_recorder()\n",
        "\n",
        "  while not terminated and time <= 500:\n",
        "      input_st, input_ac, input_rw = states[:, -block_size:, :], \\\n",
        "              actions[:, -block_size:], rewards[:, -block_size:]\n",
        "\n",
        "      output, loss = dt(input_st, input_ac, input_rw)\n",
        "      output = output[:, -1:, :].view(ac_dims)\n",
        "      probabilities = F.softmax(output[:], dim=0)\n",
        "      # ---> TODO: how to select an action\n",
        "      # select action with the greatest probability according to policy\n",
        "      action = int(torch.argmax(probabilities))\n",
        "      # One step forward\n",
        "      state, reward, terminated, _ = env.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "      desired -= reward\n",
        "      state = torch.tensor(state, dtype=torch.float, device=device).view(1, 1, st_dims)\n",
        "      states = torch.cat((states, state), dim=1)\n",
        "\n",
        "      reward = torch.tensor(desired, dtype=torch.float, device=device).view(1, rw_dims)\n",
        "      rewards = torch.cat((rewards, reward), dim=1)\n",
        "\n",
        "      action = torch.tensor(action, dtype=torch.long, device=device).view(1, 1)\n",
        "      actions = torch.cat((actions, action), dim=1)\n",
        "\n",
        "      time += 1\n",
        "\n",
        "  # Print reward\n",
        "  print(\"total_reward = {}\".format(total_reward))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e33eaf",
      "metadata": {
        "id": "04e33eaf",
        "outputId": "deaddc9d-e724-4d27-e864-020f547a3576"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0]]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = torch.zeros((1, 1,1), dtype=torch.long, device=device)\n",
        "context[:, -block_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f057646a",
      "metadata": {
        "id": "f057646a"
      },
      "outputs": [],
      "source": [
        "torch.save(dt, \"first_iter.model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# define a trigger function, return True to start recording a new video:\n",
        "episode_trigger=lambda episode_id: (episode_id%1==0)\n",
        "\n",
        "# decorate env with a video recorder:\n",
        "env = gym.wrappers.RecordVideo(env, \"./video\",\n",
        "         episode_trigger=episode_trigger, video_length=100000)\n",
        "\n",
        "env.start_video_recorder()"
      ],
      "metadata": {
        "id": "twyRRJ_sD0sH"
      },
      "id": "twyRRJ_sD0sH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "trajectory",
      "language": "python",
      "name": "trajectory"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}